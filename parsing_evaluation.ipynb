{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/metrics_dataset-traffic-test.json', 'r') as f:\n",
    "    training_data_a = json.load(f)\n",
    "\n",
    "with open('./input/metrics_dataset-domesticDeclarations.json', 'r') as f:\n",
    "    training_data_b = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = training_data_a['metrics']\n",
    "data.extend(training_data_b['metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_tags = ['TSE', 'TEE', 'TBE', 'CE','AttributeName', 'AttributeValue', 'AGR', 'GBC', 'FDE']\n",
    "\n",
    "cleaned_evaluation_data = []\n",
    "for phrase in data:\n",
    "    useful_slots = []\n",
    "    for slots in phrase['slots']:\n",
    "        if slots in useful_tags:\n",
    "            useful_slots.append(f\"{phrase['slots'][slots]}: {slots}\")\n",
    "    cleaned_evaluation_data.append((phrase['description'], useful_slots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from ppinat.ppiparser.PPIDecoder import PPIDecoder\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "text_model = './ppinat/models/TextClassification'\n",
    "time_model = './ppinat/models/TimeModel'\n",
    "count_model = './ppinat/models/CountModel'\n",
    "data_model = './ppinat/models/DataModel'\n",
    "\n",
    "text_model = AutoModelForSequenceClassification.from_pretrained(text_model)\n",
    "time_model = AutoModelForTokenClassification.from_pretrained(time_model)\n",
    "count_model = AutoModelForTokenClassification.from_pretrained(count_model)\n",
    "data_model = AutoModelForTokenClassification.from_pretrained(data_model)\n",
    "model = {\"time\": time_model, \"count\": count_model, \"data\": data_model} \n",
    "decoder = PPIDecoder(model, tokenizer, text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_outputs = []\n",
    "predicted_outputs = []\n",
    "for phrase in cleaned_evaluation_data:\n",
    "    input_text = phrase[0]\n",
    "    expected_output = \"; \".join(phrase[1])\n",
    "    expected_outputs.append(expected_output)\n",
    "\n",
    "    prediction = decoder.predict_annotation(input_text)\n",
    "    predicted_outputs.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hermetrics.levenshtein import Levenshtein\n",
    "import numpy as np\n",
    "\n",
    "def compute_slot_metric(preds, real):\n",
    "    phrases_metrics = []\n",
    "    for real_phrase, pred_phrase in zip(real,preds):\n",
    "        real_slots = real_phrase.split(\"; \") \n",
    "        pred_slots = pred_phrase.split(\"; \")\n",
    "        slots_metrics = []\n",
    "        for real_slot in real_slots:\n",
    "            real_tag = real_slot.split(\": \")[1]\n",
    "            slot_metric = 0\n",
    "            for pred_slot in pred_slots:\n",
    "                try:\n",
    "                    pred_tag = pred_slot.split(\": \")[1]\n",
    "                    if real_tag == pred_tag:\n",
    "                        real_text = real_slot.split(\": \")[0]\n",
    "                        pred_text = pred_slot.split(\": \")[0]\n",
    "                        slot_metric = (1 - Levenshtein().normalized_distance(real_text, pred_text))\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            slots_metrics.append(slot_metric)\n",
    "        phrases_metrics.append(np.mean(slots_metrics))\n",
    "    return np.mean(phrases_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_slot_metric_per_tag(preds, real):\n",
    "    results_per_tag = {}\n",
    "    for tag in useful_tags:\n",
    "        results_per_tag[tag] = []\n",
    "        \n",
    "    for real_phrase, pred_phrase in zip(real,preds):\n",
    "        real_slots = real_phrase.split(\"; \") \n",
    "        pred_slots = pred_phrase.split(\"; \")\n",
    "        for real_slot in real_slots:\n",
    "            real_tag = real_slot.split(\": \")[1]\n",
    "            matched_tag = False\n",
    "            for pred_slot in pred_slots:\n",
    "                try:\n",
    "                    pred_tag = pred_slot.split(\": \")[1]\n",
    "                    if real_tag == pred_tag:\n",
    "                        matched_tag = True\n",
    "                        real_text = real_slot.split(\": \")[0]\n",
    "                        pred_text = pred_slot.split(\": \")[0]\n",
    "                        distance = 1 - Levenshtein().normalized_distance(real_text, pred_text)\n",
    "                        results_per_tag[real_tag].append(distance)\n",
    "                except Exception as e:\n",
    "                    results_per_tag[real_tag].append(0)\n",
    "                    pass\n",
    "            if not matched_tag:\n",
    "                results_per_tag[real_tag].append(0)\n",
    "    \n",
    "    for tag in results_per_tag:\n",
    "        results_per_tag[tag] = np.mean(results_per_tag[tag])\n",
    "    return results_per_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_slot_accuracy_per_tag(preds, real):\n",
    "    results_per_tag = {}\n",
    "    for tag in useful_tags:\n",
    "        results_per_tag[tag] = []\n",
    "        \n",
    "    for real_phrase, pred_phrase in zip(real,preds):\n",
    "        real_slots = real_phrase.split(\"; \") \n",
    "        pred_slots = pred_phrase.split(\"; \")\n",
    "        for real_slot in real_slots:\n",
    "            real_tag = real_slot.split(\": \")[1]\n",
    "            matched_tag = False\n",
    "            for pred_slot in pred_slots:\n",
    "                try:\n",
    "                    pred_tag = pred_slot.split(\": \")[1]\n",
    "                    if real_tag == pred_tag:\n",
    "                        matched_tag = True\n",
    "                        real_text = real_slot.split(\": \")[0]\n",
    "                        pred_text = pred_slot.split(\": \")[0]\n",
    "                        if real_text == pred_text:\n",
    "                            results_per_tag[real_tag].append(1)\n",
    "                        else:\n",
    "                            results_per_tag[real_tag].append(0)\n",
    "                except Exception as e:\n",
    "                    results_per_tag[real_tag].append(0)\n",
    "                    pass\n",
    "            if not matched_tag:\n",
    "                results_per_tag[real_tag].append(0)\n",
    "    \n",
    "    for tag in results_per_tag:\n",
    "        results_per_tag[tag] = np.mean(results_per_tag[tag])\n",
    "    return results_per_tag\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPIBOT old models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot accuracy: 0.3660130718954248\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def compute_slot_accuracy(preds, real):\n",
    "    phrases_metrics = []\n",
    "    for real_phrase, pred_phrase in zip(real,preds):\n",
    "        real_slots = real_phrase.split(\"; \") \n",
    "        slots_metrics = []\n",
    "        for real_slot in real_slots:\n",
    "            real_tag = real_slot.split(\": \")[1]\n",
    "            slot_metric = 0\n",
    "            predicted_chunk = pred_phrase.get_chunk_by_tag(real_tag)\n",
    "            if predicted_chunk is not None:\n",
    "                real_text = real_slot.split(\": \")[0]\n",
    "                pred_text = predicted_chunk.text()\n",
    "                if real_text == pred_text:\n",
    "                    slot_metric = 1\n",
    "                else:\n",
    "                    slot_metric = 0\n",
    "            else:\n",
    "                slot_metric = 0\n",
    "            slots_metrics.append(slot_metric)\n",
    "        phrases_metrics.append(np.mean(slots_metrics))\n",
    "    return np.mean(phrases_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hermetrics.levenshtein import Levenshtein\n",
    "import numpy as np\n",
    "\n",
    "def compute_slot_metric(preds, real):\n",
    "    phrases_metrics = []\n",
    "    for real_phrase, pred_phrase in zip(real,preds):\n",
    "        real_slots = real_phrase.split(\"; \") \n",
    "        slots_metrics = []\n",
    "        for real_slot in real_slots:\n",
    "            real_tag = real_slot.split(\": \")[1]\n",
    "            slot_metric = 0\n",
    "            predicted_chunk = pred_phrase.get_chunk_by_tag(real_tag)\n",
    "            if predicted_chunk is not None:\n",
    "                real_text = real_slot.split(\": \")[0]\n",
    "                pred_text = predicted_chunk.text()\n",
    "                slot_metric = (1 - Levenshtein().normalized_distance(real_text, pred_text))\n",
    "            else:\n",
    "                slot_metric = 0\n",
    "            slots_metrics.append(slot_metric)\n",
    "        phrases_metrics.append(np.mean(slots_metrics))\n",
    "    return np.mean(phrases_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot accuracy: 0.3660130718954248\n",
      "Slot metric: 0.47673013780966694\n"
     ]
    }
   ],
   "source": [
    "print(f\"Slot accuracy: {compute_slot_accuracy(predicted_outputs, expected_outputs)}\")\n",
    "print(f\"Slot metric: {compute_slot_metric(predicted_outputs, expected_outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
