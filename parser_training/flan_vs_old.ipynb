{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/metrics_dataset-traffic-test.json', 'r') as f:\n",
    "    training_data_a = json.load(f)\n",
    "\n",
    "with open('../input/metrics_dataset-domesticDeclarations.json', 'r') as f:\n",
    "    training_data_b = json.load(f)\n",
    "\n",
    "with open('../input/metrics_dataset.json', 'r') as f:\n",
    "    training_data_c = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = training_data_a['metrics']\n",
    "data.extend(training_data_b['metrics'])\n",
    "data.extend(training_data_c['metrics'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flan Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"-time: average time from opened to closed, Elapsed time between an action requirement is notified until the action is performed\\n-count: number of activities reopened, number of users within the black list\\n-data: Average of declared costs detailed per trip, the value of state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1/81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppibot/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 2/81\n",
      "Predicting 3/81\n",
      "Predicting 4/81\n",
      "Predicting 5/81\n",
      "Predicting 6/81\n",
      "Predicting 7/81\n",
      "Predicting 8/81\n",
      "Predicting 9/81\n",
      "Predicting 10/81\n",
      "Predicting 11/81\n",
      "Predicting 12/81\n",
      "Predicting 13/81\n",
      "Predicting 14/81\n",
      "Predicting 15/81\n",
      "Predicting 16/81\n",
      "Predicting 17/81\n",
      "Predicting 18/81\n",
      "Predicting 19/81\n",
      "Predicting 20/81\n",
      "Predicting 21/81\n",
      "Predicting 22/81\n",
      "Predicting 23/81\n",
      "Predicting 24/81\n",
      "Predicting 25/81\n",
      "Predicting 26/81\n",
      "Predicting 27/81\n",
      "Predicting 28/81\n",
      "Predicting 29/81\n",
      "Predicting 30/81\n",
      "Predicting 31/81\n",
      "Predicting 32/81\n",
      "Predicting 33/81\n",
      "Predicting 34/81\n",
      "Predicting 35/81\n",
      "Predicting 36/81\n",
      "Predicting 37/81\n",
      "Predicting 38/81\n",
      "Predicting 39/81\n",
      "Predicting 40/81\n",
      "Predicting 41/81\n",
      "Predicting 42/81\n",
      "Predicting 43/81\n",
      "Predicting 44/81\n",
      "Predicting 45/81\n",
      "Predicting 46/81\n",
      "Predicting 47/81\n",
      "Predicting 48/81\n",
      "Predicting 49/81\n",
      "Predicting 50/81\n",
      "Predicting 51/81\n",
      "Predicting 52/81\n",
      "Predicting 53/81\n",
      "Predicting 54/81\n",
      "Predicting 55/81\n",
      "Predicting 56/81\n",
      "Predicting 57/81\n",
      "Predicting 58/81\n",
      "Predicting 59/81\n",
      "Predicting 60/81\n",
      "Predicting 61/81\n",
      "Predicting 62/81\n",
      "Predicting 63/81\n",
      "Predicting 64/81\n",
      "Predicting 65/81\n",
      "Predicting 66/81\n",
      "Predicting 67/81\n",
      "Predicting 68/81\n",
      "Predicting 69/81\n",
      "Predicting 70/81\n",
      "Predicting 71/81\n",
      "Predicting 72/81\n",
      "Predicting 73/81\n",
      "Predicting 74/81\n",
      "Predicting 75/81\n",
      "Predicting 76/81\n",
      "Predicting 77/81\n",
      "Predicting 78/81\n"
     ]
    }
   ],
   "source": [
    "real_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "counter = 0\n",
    "for phrase in data:\n",
    "    if phrase['type'] not in ['time', 'count', 'data']:\n",
    "        continue\n",
    "    counter += 1\n",
    "    print(f\"Predicting {counter}/{len(data)}\")\n",
    "    real_labels.append(phrase['type'])\n",
    "\n",
    "    prompt = f\"Given this sentence:\\n{phrase['description']}\\nChoose the correct label:\\n-time\\n-count\\n-data\\n\\nEXAMPLES\\n{examples}\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(input_ids)\n",
    "    predicted_labels.append(tokenizer.decode(outputs[0], skip_special_tokens=True))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9487179487179487\n",
      "Precision: 0.9648879648879648\n",
      "Recall: 0.8162393162393163\n",
      "F1: 0.8625356125356126\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(real_labels, predicted_labels, average='macro')\n",
    "acc = accuracy_score(real_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       count       0.95      0.95      0.95        39\n",
      "        data       1.00      0.50      0.67         4\n",
      "        time       0.95      1.00      0.97        35\n",
      "\n",
      "    accuracy                           0.95        78\n",
      "   macro avg       0.96      0.82      0.86        78\n",
      "weighted avg       0.95      0.95      0.94        78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(real_labels, predicted_labels)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 1/81\n",
      "Predicting 2/81\n",
      "Predicting 3/81\n",
      "Predicting 4/81\n",
      "Predicting 5/81\n",
      "Predicting 6/81\n",
      "Predicting 7/81\n",
      "Predicting 8/81\n",
      "Predicting 9/81\n",
      "Predicting 10/81\n",
      "Predicting 11/81\n",
      "Predicting 12/81\n",
      "Predicting 13/81\n",
      "Predicting 14/81\n",
      "Predicting 15/81\n",
      "Predicting 16/81\n",
      "Predicting 17/81\n",
      "Predicting 18/81\n",
      "Predicting 19/81\n",
      "Predicting 20/81\n",
      "Predicting 21/81\n",
      "Predicting 22/81\n",
      "Predicting 23/81\n",
      "Predicting 24/81\n",
      "Predicting 25/81\n",
      "Predicting 26/81\n",
      "Predicting 27/81\n",
      "Predicting 28/81\n",
      "Predicting 29/81\n",
      "Predicting 30/81\n",
      "Predicting 31/81\n",
      "Predicting 32/81\n",
      "Predicting 33/81\n",
      "Predicting 34/81\n",
      "Predicting 35/81\n",
      "Predicting 36/81\n",
      "Predicting 37/81\n",
      "Predicting 38/81\n",
      "Predicting 39/81\n",
      "Predicting 40/81\n",
      "Predicting 41/81\n",
      "Predicting 42/81\n",
      "Predicting 43/81\n",
      "Predicting 44/81\n",
      "Predicting 45/81\n",
      "Predicting 46/81\n",
      "Predicting 47/81\n",
      "Predicting 48/81\n",
      "Predicting 49/81\n",
      "Predicting 50/81\n",
      "Predicting 51/81\n",
      "Predicting 52/81\n",
      "Predicting 53/81\n",
      "Predicting 54/81\n",
      "Predicting 55/81\n",
      "Predicting 56/81\n",
      "Predicting 57/81\n",
      "Predicting 58/81\n",
      "Predicting 59/81\n",
      "Predicting 60/81\n",
      "Predicting 61/81\n",
      "Predicting 62/81\n",
      "Predicting 63/81\n",
      "Predicting 64/81\n",
      "Predicting 65/81\n",
      "Predicting 66/81\n",
      "Predicting 67/81\n",
      "Predicting 68/81\n",
      "Predicting 69/81\n",
      "Predicting 70/81\n",
      "Predicting 71/81\n",
      "Predicting 72/81\n",
      "Predicting 73/81\n",
      "Predicting 74/81\n",
      "Predicting 75/81\n",
      "Predicting 76/81\n",
      "Predicting 77/81\n",
      "Predicting 78/81\n",
      "Accuracy: 0.9487179487179487\n",
      "Precision: 0.8483983983983984\n",
      "Recall: 0.8910256410256411\n",
      "F1: 0.8666501935590148\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"../ppinat/models/TextClassification\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "real_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "counter = 0\n",
    "for phrase in data:\n",
    "    if phrase['type'] not in ['time', 'count', 'data']:\n",
    "        continue\n",
    "    counter += 1\n",
    "    print(f\"Predicting {counter}/{len(data)}\")\n",
    "    real_labels.append(phrase['type'])\n",
    "\n",
    "    words = phrase['description'].split()\n",
    "    tokens  = tokenizer(words, return_tensors='pt', truncation=True, is_split_into_words=True)\n",
    "\n",
    "    metric_type = model(**tokens)[\"logits\"].argmax(-1).tolist()[0]\n",
    "    if metric_type == 0:\n",
    "        type = \"time\"\n",
    "    elif metric_type == 1:\n",
    "        type = \"count\"\n",
    "    else:\n",
    "        type = \"data\"\n",
    "\n",
    "    predicted_labels.append(type)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(real_labels, predicted_labels, average='macro')\n",
    "acc = accuracy_score(real_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       count       0.97      0.92      0.95        39\n",
      "        data       0.60      0.75      0.67         4\n",
      "        time       0.97      1.00      0.99        35\n",
      "\n",
      "    accuracy                           0.95        78\n",
      "   macro avg       0.85      0.89      0.87        78\n",
      "weighted avg       0.95      0.95      0.95        78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(real_labels, predicted_labels)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
