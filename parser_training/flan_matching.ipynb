{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 150370/150370 [00:13<00:00, 11131.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount, org:resource, dismissal, concept:name, vehicleClass, totalPaymentAmount, lifecycle:transition, time:timestamp, article, points, case:concept:name, expense, notificationType, lastSent, paymentAmount, matricola\n",
      "Create Fine, Send Fine, Insert Fine Notification, Add penalty, Send for Credit Collection, Payment, Insert Date Appeal to Prefecture, Send Appeal to Prefecture, Receive Result Appeal from Prefecture, Notify Result Appeal to Offender, Appeal to Judge\n"
     ]
    }
   ],
   "source": [
    "import pm4py\n",
    "\n",
    "log = pm4py.read_xes(\"../input/event_logs/Road_Traffic_Fine_Management_Process.xes\")\n",
    "df = pm4py.convert_to_dataframe(log)\n",
    "\n",
    "column_names = list(df.columns)\n",
    "print(', '.join(column_names))\n",
    "\n",
    "event_names = list(df[\"concept:name\"].unique())\n",
    "print(', '.join(event_names)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppibot/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Fine: yes\n",
      "Send Fine: yes\n",
      "Insert Fine Notification: yes\n",
      "Add penalty: yes\n",
      "Send for Credit Collection: no\n",
      "Payment: no\n",
      "Insert Date Appeal to Prefecture: no\n",
      "Send Appeal to Prefecture: no\n",
      "Receive Result Appeal from Prefecture: no\n",
      "Notify Result Appeal to Offender: no\n",
      "Appeal to Judge: no\n"
     ]
    }
   ],
   "source": [
    "#Funciona considerablemente bien.\n",
    "#Problema: si da yes a varias actividades\n",
    "for event_name in event_names:\n",
    "    paraphrasing_prompt = f\"notify fine to offender\\n{event_name}\\nAre these two sentences paraphrases of each other?\"\n",
    "    inputs = tokenizer(paraphrasing_prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(inputs)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"{event_name}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send Fine\n"
     ]
    }
   ],
   "source": [
    "#Prueba con zero-shot: devuelve un resultado pero no es el correcto\n",
    "options_text = \"\\n-\".join(event_names)\n",
    "options_text = \"-\" + options_text\n",
    "prompt = f\"Sentence:\\nnotify fine to offender\\nWhat is the most similar paraphrase?\\nOPTIONS:\\n{options_text}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(inputs, max_length=1000)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Próximo: Probar a poner fewshot con parafrases de cada nombre de actividad\n",
    "¿Se usa actualmente sentence transformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = '''\n",
    "- Create Fine: Generate the fine, Establish Fine, Create the fine\n",
    "- Send Fine: Dispatch the fine, Transmit the fine, Forward the fine\n",
    "- Insert Fine Notification: Enter the notification for the fine, Add the fine notice, Include the fine notification\n",
    "- Add Penalty: Impose the penalty, Levy the penalty, Apply the penalty\n",
    "- Send for Credit Collection: Forward for debt collection, Send for payment recovery, Transfer for credit recovery\n",
    "- Payment: Remittance, Settlement, Clearing of dues\n",
    "- Insert Date Appeal to Prefecture: Enter the date of the appeal to the prefecture, Add the date for the prefecture appeal, Input the appeal date to the regional authority\n",
    "- Send Appeal to Prefecture: Dispatch the appeal to the prefecture, Transmit the plea to the regional authority, Forward the appeal to the regional authority\n",
    "- Receive Result Appeal from Prefecture: Obtain the result of the appeal from the prefecture, Receive the appeal outcome from the regional authority, Get the appeal decision from the regional authority\n",
    "- Notify Result Appeal to Offender: Inform the offender of the appeal result, Advise the offender of the appeal outcome, Notify the appellant of the appeal decision\n",
    "- Appeal to Judge: Contest before the judge, Challenge the decision to the judicial authority, Plead to the judge\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appeal to Judge\n"
     ]
    }
   ],
   "source": [
    "# Prueba con few-shot: de vez en cuando devuelve el resultado correcto\n",
    "options_text = \"\\n-\".join(event_names)\n",
    "options_text = \"-\" + options_text\n",
    "prompt = f\"Sentence:\\nfines with appeal\\nWhat is the most similar activity name?\\nOPTIONS:\\n{options_text}\\n\\nEXAMPLES:\\n{examples}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(inputs, max_length=1000)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/metrics_dataset-traffic-test.json', 'r') as f:\n",
    "    training_data_a = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = training_data_a['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_dict = {   \n",
    "    'TSE': 'from_condition',\n",
    "    'TEE': 'to_condition',\n",
    "    'CE': 'count_condition',\n",
    "    'TBE': ['from_condition', 'to_condition']\n",
    "}\n",
    "\n",
    "parsing_values = []\n",
    "real_matchings = []\n",
    "for example in data:\n",
    "    slots = example['slots']\n",
    "    try:\n",
    "        goldstandard = example['goldstandard']\n",
    "        matching = goldstandard['traffic']\n",
    "    except:\n",
    "        continue\n",
    "    for slot in slots:\n",
    "        if slot in slot_dict:\n",
    "            matching_name = slot_dict[slot]\n",
    "            try:\n",
    "                if isinstance(matching_name, list):\n",
    "                    matching_values = []\n",
    "                    for name in matching_name:\n",
    "                        matching_value = matching[name]\n",
    "                        if \"attribute\" in matching_value:\n",
    "                            if matching_value['attribute'] == 'ACTIVITY':\n",
    "                                real_matching = matching_value['value']\n",
    "                                slot_value = slots[slot]\n",
    "                                parsing_values.append(slot_value)\n",
    "                                real_matchings.append(real_matching)\n",
    "                                matching_values.append(real_matching)\n",
    "                                break       \n",
    "                        \n",
    "                else:  \n",
    "                    matching_value = matching[matching_name]\n",
    "                    if matching_value['attribute'] == 'ACTIVITY':\n",
    "                        real_matching = matching_value['value']\n",
    "                        slot_value = slots[slot]\n",
    "                        parsing_values.append(slot_value)\n",
    "                        real_matchings.append(real_matching)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: Insert Date Appeal to Prefecture, Predicted: Send Appeal to Prefecture\n",
      "Expected: Payment, Predicted: Send Appeal to Prefecture\n",
      "Expected: Notify Result Appeal to Offender, Predicted: Insert Fine Notification\n",
      "Expected: Insert Fine Notification, Predicted: Send Fine\n",
      "Expected: Insert Fine Notification, Predicted: Receive Result Appeal from Prefecture\n",
      "Expected: Insert Date Appeal to Prefecture, Predicted: Send Appeal to Prefecture\n",
      "Expected: Add penalty, Predicted: Create Fine\n",
      "Expected: Add penalty, Predicted: Create Fine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicteds = []\n",
    "for parsing_value, expected_matching in zip(parsing_values, real_matchings):\n",
    "    options_text = \"\\n-\".join(event_names)\n",
    "    options_text = \"-\" + options_text\n",
    "    prompt = f\"Sentence:\\n{parsing_value}\\nWhat is the most similar activity name?\\nOPTIONS:\\n{options_text}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(inputs, max_length=1000)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predicteds.append(result)\n",
    "    if result != expected_matching:\n",
    "        print(f\"Expected: {expected_matching}, Predicted: {result}\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(real_matchings, predicteds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing: appeal\n",
      "Expected: Insert Date Appeal to Prefecture, Predicted: Appeal to Judge\n",
      "\n",
      "Parsing: notification\n",
      "Expected: Notify Result Appeal to Offender, Predicted: Insert Fine Notification\n",
      "\n",
      "Parsing: notify fine to offender\n",
      "Expected: Insert Fine Notification, Predicted: Notify Result Appeal to Offender\n",
      "\n",
      "Parsing: fines with appeal\n",
      "Expected: Insert Date Appeal to Prefecture, Predicted: Appeal to Judge\n",
      "\n",
      "Parsing: the prefecture to solve an appeal\n",
      "Expected: Send Appeal to Prefecture, Predicted: Receive Result Appeal from Prefecture: Obtain the result of the appeal from the prefecture, Receive the appeal outcome from the regional authority, Get the appeal decision from the regional authority\n",
      "\n",
      "Parsing: the prefecture\n",
      "Expected: Send Appeal to Prefecture, Predicted: Receive Result Appeal from Prefecture: Obtain the result of the appeal from the prefecture, Receive the appeal outcome from the regional authority, Get the appeal decision from the regional authority\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With 3 examples\n",
    "\n",
    "predicteds = []\n",
    "for parsing_value, expected_matching in zip(parsing_values, real_matchings):\n",
    "    options_text = \"\\n-\".join(event_names)\n",
    "    options_text = \"-\" + options_text\n",
    "    prompt = f\"Sentence:\\n{parsing_value}\\nWhat is the most similar activity name?\\n\\nOPTIONS:\\n{options_text}\\n\\nEXAMPLES:\\n{examples}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(inputs, max_length=1000)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predicteds.append(result)\n",
    "    if result != expected_matching:\n",
    "        print(f\"Parsing: {parsing_value}\")\n",
    "        print(f\"Expected: {expected_matching}, Predicted: {result}\")\n",
    "        print()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(real_matchings, predicteds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 10500/10500 [00:00<00:00, 10830.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case:Amount\n"
     ]
    }
   ],
   "source": [
    "# Matching AttributeName and GBC with names of attributes\n",
    "log = pm4py.read_xes(\"../input/event_logs/DomesticDeclarations.xes\")\n",
    "df = pm4py.convert_to_dataframe(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case:Amount\n"
     ]
    }
   ],
   "source": [
    "attribute_names = list(df.columns)\n",
    "\n",
    "slot_value =\"costs\"\n",
    "options_text = \"\\n-\".join(attribute_names)\n",
    "options_text = \"-\" + options_text\n",
    "prompt = f\"Sentence:\\n{slot_value}\\nWhat is the most similar attribute name?\\n\\nOPTIONS:\\n{options_text}\\n\\nEXAMPLES:\\n{examples}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(inputs, max_length=1000)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
