{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./parsing-model-2023-06-09 13:28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/metrics_dataset-traffic-test.json', 'r') as f:\n",
    "    training_data_a = json.load(f)\n",
    "\n",
    "with open('../input/metrics_dataset-domesticDeclarations.json', 'r') as f:\n",
    "    training_data_b = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = training_data_a['metrics']\n",
    "data.extend(training_data_b['metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_tags = ['TSE', 'TEE', 'TBE', 'CE','AttributeName', 'AttributeValue', 'AGR', 'GBC', 'FDE']\n",
    "\n",
    "cleaned_evaluation_data = []\n",
    "for phrase in data:\n",
    "    useful_slots = []\n",
    "    for slots in phrase['slots']:\n",
    "        if slots in useful_tags:\n",
    "            useful_slots.append(f\"{phrase['slots'][slots]}: {slots}\")\n",
    "    cleaned_evaluation_data.append((phrase['description'], useful_slots))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_outputs = []\n",
    "predicted_outputs = []\n",
    "for phrase in cleaned_evaluation_data:\n",
    "    input_text = phrase[0]\n",
    "    expected_output = \"; \".join(phrase[1])\n",
    "    expected_outputs.append(expected_output)\n",
    "\n",
    "    prompt = f\"Sentence: {input_text}\\nAvailable Tags: {', '.join(useful_tags)}\"\n",
    "    encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(input_ids, max_length=1000)\n",
    "    predicted_outputs.append(tokenizer.decode(outputs[0], skip_special_tokens=True)) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hermetrics.levenshtein import Levenshtein\n",
    "import numpy as np\n",
    "\n",
    "def compute_slot_metric(preds, real):\n",
    "    phrases_metrics = []\n",
    "    for real_phrase, pred_phrase in zip(real,preds):\n",
    "        real_slots = real_phrase.split(\"; \") \n",
    "        pred_slots = pred_phrase.split(\"; \")\n",
    "        slots_metrics = []\n",
    "        for real_slot in real_slots:\n",
    "            real_tag = real_slot.split(\": \")[1]\n",
    "            slot_metric = 0\n",
    "            for pred_slot in pred_slots:\n",
    "                try:\n",
    "                    pred_tag = pred_slot.split(\": \")[1]\n",
    "                    if real_tag == pred_tag:\n",
    "                        real_text = real_slot.split(\": \")[0]\n",
    "                        pred_text = pred_slot.split(\": \")[0]\n",
    "                        slot_metric = (1 - Levenshtein().normalized_distance(real_text, pred_text))\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            slots_metrics.append(slot_metric)\n",
    "        phrases_metrics.append(np.mean(slots_metrics))\n",
    "    return np.mean(phrases_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_slot_accuracy(preds, real):\n",
    "    phrases_metrics = []\n",
    "    for real_phrase, pred_phrase in zip(real,preds):\n",
    "        real_slots = real_phrase.split(\"; \") \n",
    "        pred_slots = pred_phrase.split(\"; \")\n",
    "        slots_metrics = []\n",
    "        for real_slot in real_slots:\n",
    "            real_tag = real_slot.split(\": \")[1]\n",
    "            slot_metric = 0\n",
    "            for pred_slot in pred_slots:\n",
    "                try:\n",
    "                    pred_tag = pred_slot.split(\": \")[1]\n",
    "                    if real_tag == pred_tag:\n",
    "                        real_text = real_slot.split(\": \")[0]\n",
    "                        pred_text = pred_slot.split(\": \")[0]\n",
    "                        if real_text == pred_text:\n",
    "                            slot_metric = 1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            slots_metrics.append(slot_metric)\n",
    "        phrases_metrics.append(np.mean(slots_metrics))\n",
    "    return np.mean(phrases_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_slot_metric_per_tag(preds, real):\n",
    "    results_per_tag = {}\n",
    "    for tag in useful_tags:\n",
    "        results_per_tag[tag] = []\n",
    "        \n",
    "    for real_phrase, pred_phrase in zip(real,preds):\n",
    "        real_slots = real_phrase.split(\"; \") \n",
    "        pred_slots = pred_phrase.split(\"; \")\n",
    "        for real_slot in real_slots:\n",
    "            real_tag = real_slot.split(\": \")[1]\n",
    "            matched_tag = False\n",
    "            for pred_slot in pred_slots:\n",
    "                try:\n",
    "                    pred_tag = pred_slot.split(\": \")[1]\n",
    "                    if real_tag == pred_tag:\n",
    "                        matched_tag = True\n",
    "                        real_text = real_slot.split(\": \")[0]\n",
    "                        pred_text = pred_slot.split(\": \")[0]\n",
    "                        distance = 1 - Levenshtein().normalized_distance(real_text, pred_text)\n",
    "                        results_per_tag[real_tag].append(distance)\n",
    "                except Exception as e:\n",
    "                    results_per_tag[real_tag].append(0)\n",
    "                    pass\n",
    "            if not matched_tag:\n",
    "                results_per_tag[real_tag].append(0)\n",
    "    \n",
    "    for tag in results_per_tag:\n",
    "        results_per_tag[tag] = np.mean(results_per_tag[tag])\n",
    "    return results_per_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_slot_accuracy_per_tag(preds, real):\n",
    "    results_per_tag = {}\n",
    "    for tag in useful_tags:\n",
    "        results_per_tag[tag] = []\n",
    "        \n",
    "    for real_phrase, pred_phrase in zip(real,preds):\n",
    "        real_slots = real_phrase.split(\"; \") \n",
    "        pred_slots = pred_phrase.split(\"; \")\n",
    "        for real_slot in real_slots:\n",
    "            real_tag = real_slot.split(\": \")[1]\n",
    "            matched_tag = False\n",
    "            for pred_slot in pred_slots:\n",
    "                try:\n",
    "                    pred_tag = pred_slot.split(\": \")[1]\n",
    "                    if real_tag == pred_tag:\n",
    "                        matched_tag = True\n",
    "                        real_text = real_slot.split(\": \")[0]\n",
    "                        pred_text = pred_slot.split(\": \")[0]\n",
    "                        if real_text == pred_text:\n",
    "                            results_per_tag[real_tag].append(1)\n",
    "                        else:\n",
    "                            results_per_tag[real_tag].append(0)\n",
    "                except Exception as e:\n",
    "                    results_per_tag[real_tag].append(0)\n",
    "                    pass\n",
    "            if not matched_tag:\n",
    "                results_per_tag[real_tag].append(0)\n",
    "    \n",
    "    for tag in results_per_tag:\n",
    "        results_per_tag[tag] = np.mean(results_per_tag[tag])\n",
    "    return results_per_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.43137254901960786\n",
      "Slot Metric: 0.7701802803981406\n",
      "Slot Accuracy Metric: 0.6045751633986928\n",
      "Slot Accuracy Per Tag: {'TSE': 0.4, 'TEE': 0.42857142857142855, 'TBE': 0.2222222222222222, 'CE': 0.7142857142857143, 'AttributeName': 0.5, 'AGR': 0.7142857142857143, 'GBC': 1.0, 'FDE': 1.0}\n",
      "Slot Metric Per Tag: {'TSE': 0.7815325670498084, 'TEE': 0.6037305244678517, 'TBE': 0.6618205868205869, 'CE': 0.8399143031902588, 'AttributeName': 0.5, 'AGR': 0.789010989010989, 'GBC': 1.0, 'FDE': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(expected_outputs, predicted_outputs)}\")\n",
    "print(f\"Slot Metric: {compute_slot_metric(predicted_outputs, expected_outputs)}\")\n",
    "print(f\"Slot Accuracy Metric: {compute_slot_accuracy(predicted_outputs, expected_outputs)}\")\n",
    "print(f\"Slot Accuracy Per Tag: {compute_slot_accuracy_per_tag(predicted_outputs, expected_outputs)}\")\n",
    "print(f\"Slot Metric Per Tag: {compute_slot_metric_per_tag(predicted_outputs, expected_outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: AGR; declared costs detailed: CE; trip: GBC\n"
     ]
    }
   ],
   "source": [
    "example = \"Average of declared costs detailed per trip\"\n",
    "prompt = f\"Sentence: {example}\\nAvailable Tags: {', '.join(useful_tags)}\"\n",
    "encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=1000)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
